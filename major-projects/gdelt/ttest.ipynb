{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e58b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG (tune here, not everywhere)\n",
    "# ==================================================\n",
    "MAX_GDELT_ROWS = 5000        # safe cap\n",
    "MAX_ARTICLES = 80            # headlines to fetch\n",
    "REQUEST_TIMEOUT = 5\n",
    "HEADLINE_DELAY = 0.25        # polite scraping delay\n",
    "MIN_CLUSTER_SIZE = 5\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 1. FAST GDELT FETCH (lastupdate.txt)\n",
    "# ==================================================\n",
    "def fetch_gdelt_latest():\n",
    "    lastupdate_url = \"http://data.gdeltproject.org/gdeltv2/lastupdate.txt\"\n",
    "    text = requests.get(lastupdate_url, timeout=10).text\n",
    "\n",
    "    mentions_url = None\n",
    "    for line in text.splitlines():\n",
    "        if \"mentions\" in line:\n",
    "            mentions_url = line.split()[-1]\n",
    "            break\n",
    "\n",
    "    if not mentions_url:\n",
    "        raise RuntimeError(\"No mentions file found in lastupdate.txt\")\n",
    "\n",
    "    print(f\"[INFO] Fetching: {mentions_url}\")\n",
    "\n",
    "    zip_bytes = requests.get(mentions_url, timeout=15).content\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:\n",
    "        csv_name = z.namelist()[0]\n",
    "        with z.open(csv_name) as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=\"\\t\",\n",
    "                header=None,\n",
    "                encoding=\"latin-1\",\n",
    "                low_memory=False\n",
    "            )\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(df)} GDELT rows\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 2. HEADLINE EXTRACTION (LIGHT + SAFE)\n",
    "# ==================================================\n",
    "def fetch_article_title(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        r = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        og = soup.find(\"meta\", property=\"og:title\")\n",
    "        if og and og.get(\"content\"):\n",
    "            return og[\"content\"].strip()\n",
    "\n",
    "        if soup.title and soup.title.text:\n",
    "            return soup.title.text.strip()\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_real_headlines(df):\n",
    "    urls = df[5].dropna().unique()[:MAX_ARTICLES]\n",
    "\n",
    "    headlines = []\n",
    "\n",
    "    print(\"[INFO] Fetching article headlines...\")\n",
    "\n",
    "    for url in urls:\n",
    "        title = fetch_article_title(url)\n",
    "        if title and len(title.split()) > 4:\n",
    "            headlines.append(title)\n",
    "        time.sleep(HEADLINE_DELAY)\n",
    "\n",
    "    print(f\"[INFO] Collected {len(headlines)} clean headlines\")\n",
    "    return headlines\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 3. EMBEDDINGS (LOAD MODEL ONCE)\n",
    "# ==================================================\n",
    "def embed_texts(texts, model):\n",
    "    print(\"[INFO] Creating embeddings...\")\n",
    "    return model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 4. UNSUPERVISED CLUSTERING\n",
    "# ==================================================\n",
    "def cluster_embeddings(embeddings):\n",
    "    print(\"[INFO] Clustering...\")\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=MIN_CLUSTER_SIZE,\n",
    "        metric=\"euclidean\"\n",
    "    )\n",
    "    return clusterer.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 5. PRINT DAILY HIGHLIGHTS\n",
    "# ==================================================\n",
    "def print_highlights(texts, embeddings, labels):\n",
    "    clusters = {}\n",
    "\n",
    "    for text, emb, label in zip(texts, embeddings, labels):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        clusters.setdefault(label, []).append((text, emb))\n",
    "\n",
    "    print(f\"\\nðŸ“° FOUND {len(clusters)} NEWS CLUSTERS:\\n\")\n",
    "\n",
    "    for label, items in clusters.items():\n",
    "        texts_, embs_ = zip(*items)\n",
    "        center = np.mean(embs_, axis=0)\n",
    "        sims = np.dot(embs_, center)\n",
    "        highlight = texts_[np.argmax(sims)]\n",
    "\n",
    "        print(f\"ðŸŸ¦ CLUSTER {label} ({len(items)} articles)\")\n",
    "        print(f\"â†’ {highlight}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# 6. MAIN PIPELINE (FAST + SAFE)\n",
    "# ==================================================\n",
    "def run_pipeline():\n",
    "    df = fetch_gdelt_latest()\n",
    "\n",
    "    # SAFE SAMPLING (no crash ever)\n",
    "    sample_size = min(MAX_GDELT_ROWS, len(df))\n",
    "    df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "    texts = extract_real_headlines(df)\n",
    "\n",
    "    if len(texts) < 10:\n",
    "        print(\"[WARN] Not enough articles to cluster.\")\n",
    "        return\n",
    "\n",
    "    # Load embedding model once\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    embeddings = embed_texts(texts, model)\n",
    "    labels = cluster_embeddings(embeddings)\n",
    "    print_highlights(texts, embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6795cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching: http://data.gdeltproject.org/gdeltv2/20251217143000.mentions.CSV.zip\n",
      "[INFO] Loaded 3636 GDELT rows\n",
      "[INFO] Fetching article headlines...\n",
      "[INFO] Collected 65 clean headlines\n",
      "[INFO] Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff60dbacc4f4e3db67ac30d7753aeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Clustering...\n",
      "\n",
      "ðŸ“° FOUND 0 NEWS CLUSTERS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff798205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
